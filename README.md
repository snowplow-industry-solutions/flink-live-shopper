# Snowplow Live Shopper Demo

This project demonstrates real-time user behavior analytics using Snowplow event tracking, processed by Apache Flink,
and visualized through a demo e-commerce application.

## Overview

The system captures user interactions (product views, cart actions, purchases, etc.) from a demo Next.js e-commerce
store using the Snowplow JavaScript tracker. These events are sent to a Snowplow collector, ingested into Kafka, and
then processed in real-time by an Apache Flink pipeline. The Flink pipeline calculates various metrics and features based
on user behavior within different time windows (rolling and session windows), including purchase history analysis. The
results (metrics) are stored in Redis for potential downstream use or visualization.

## Key Technologies

* **Snowplow:** Event tracking pipeline (Collector, Enrich, Kafka sink).
* **Apache Flink:** Stream processing engine for real-time analytics.
* **Apache Kafka:** Message broker for decoupling event producers and consumers.
* **Redis:** In-memory data store for storing computed metrics.
* **Next.js:** Framework for the demo e-commerce application.
* **Docker & Docker Compose:** Containerization for easy setup and deployment of all services.
* **AKHQ:** Web UI for Kafka management and inspection.
* **Redis Insight:** Web UI for Redis data visualization and management.
* **Grafana:** Visualization tool for monitoring and analyzing metrics.

## Project Structure

* `apps/`: Contains the Flink processing application (`flink/`) and the demo e-commerce store (`ecommerce/`).
* `infra/`: Contains Docker Compose configurations for the infrastructure components (Snowplow, Kafka, Redis,
  Localstack).
* `docker-compose.yaml`: Main Docker Compose file including all services.
* `up.sh`: Convenience script to initialize and start all services.

## Running the Project

1. **Prerequisites:**
    * Docker and Docker Compose installed.
    * Git installed (for submodules).

2. **Clone the Repository (if you haven't already):**
   ```bash
   git clone <repository-url>
   cd <repository-directory>
   ```

3. **Initialize and Start Services:**
   Run the provided script. This will:
    * Ensure `.env` files exist (copying from examples if needed).
    * Update Git submodules.
    * Start all services defined in the Docker Compose files in detached mode.

   ```bash
   ./up.sh
   ```

4. **Access Services:**
   Once the script completes successfully, the services will be available at:
    * **üõí Ecommerce Store:** [http://localhost:3000](http://localhost:3000)
    * **üëÅÔ∏è AKHQ (Kafka UI):** [http://localhost:8085](http://localhost:8085)
    * **üìä Redis Insight:** [http://localhost:5540](http://localhost:5540)
    * **üìà Grafana:** [http://localhost:3001](http://localhost:3001/d/-0rFuzoZk/flink-metrics-overview?orgId=1&refresh=5s)
    * **üì° Snowplow Collector:** Receives events at `http://localhost:9090`
    * **üõ†Ô∏è Kafka Brokers:** Accessible internally at `kafka:9092` and externally at `localhost:19092` (check
      `infra/kafka/docker-compose.yaml` for exact external port if needed).

5. **Stopping Services:**
   ```bash
   docker compose down
   ```

## Development

* The Flink job (`SnowplowAnalyticsPipeline.java`) processes events based on different windowing strategies:
    * Rolling windows for product views, category interactions, purchase history, and cart behavior.
    * Session windows for overall session analysis.
* Metrics generated by Flink are stored as key-value pairs in Redis.
* Events can be monitored in Kafka using AKHQ.
* Redis data can be inspected using Redis Insight. 